{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"develop/crd/","title":"develop new crd","text":"<ol> <li> <p>define CRD in pkg/k8s/apis/kdoctor.io/v1beta1/xx_types.go    add role to pkg/k8s/apis/kdoctor.io/v1beta1/rbac.go</p> </li> <li> <p>make update_openapi_sdk</p> </li> <li> <p>add crd to MutatingWebhookConfiguration and ValidatingWebhookConfiguration in charts/templates/tls.yaml </p> </li> <li> <p>add your crd to charts/template/role.yaml</p> </li> <li> <p>implement the interface pkg/pluginManager/types in pkg/plugins/xxxx    register your interface in pkg/pluginManager/types/manager.go</p> </li> </ol> <p>the plugin manager will auto help plugins to finish following jobs:</p> <ol> <li> <p>schedule task and call plugin to implement each round task</p> </li> <li> <p>collect all report and save to controller disc</p> </li> <li> <p>summarize each round result and update to CRD</p> </li> </ol>"},{"location":"develop/dev/","title":"develop","text":""},{"location":"develop/dev/#local-develop","title":"local develop","text":"<ol> <li> <p><code>make build_local_image</code></p> </li> <li> <p><code>make e2e_init</code></p> </li> <li> <p><code>make e2e_run</code></p> </li> <li> <p>check proscope, browser vists http://NodeIP:4040</p> </li> <li> <p>apply cr</p> <pre><code>cat &lt;&lt;EOF &gt; mybook.yaml\napiVersion: kdoctor.io/v1beta1\nkind: Mybook\nmetadata:\n  name: test\nspec:\n  ipVersion: 4\n  subnet: \"1.0.0.0/8\"\nEOF\nkubectl apply -f mybook.yaml\n</code></pre> </li> </ol>"},{"location":"develop/dev/#chart-develop","title":"chart develop","text":"<p>helm repo add rock https://kdoctor-io.github.io/kdoctor/</p>"},{"location":"develop/release/","title":"workflow for release","text":""},{"location":"develop/release/#pre-steps","title":"pre-steps","text":"<ul> <li> <p>update 'version' and 'appVersion' filed in 'charts/*/Chart.yaml'</p> </li> <li> <p>update version in '/VERSION'</p> </li> <li> <p>a version tag should be set on right branch. The version should go with </p> <ul> <li> <p>v0.1.0-rc0</p> </li> <li> <p>v0.1.0-rc1</p> </li> <li> <p>v0.1.0</p> </li> <li> <p>v0.1.1</p> </li> <li> <p>v0.1.2</p> </li> <li> <p>v0.2.0-rc0</p> </li> <li> <p>v0.2.0</p> </li> </ul> </li> </ul>"},{"location":"develop/release/#push-a-version-tag","title":"push a version tag","text":"<p>If a tag vx.x.x is pushed , the following steps will automatically run:</p> <ol> <li> <p>check the tag name is same with '/VERSION'</p> </li> <li> <p>create a branch named 'release-vx.x.x'</p> </li> <li> <p>build the images with the pushed tag, and push to ghcr registry</p> </li> <li> <p>generate the changelog by historical PR labeled as \"pr/release/*\"</p> <p>submit the changelog file to directory 'changelogs' of branch 'github_pages', with PR labeled as \"pr/release/robot_update_githubpage\".</p> <p>changelogs is generated by historical PR label:</p> <p>label \"pr/release/feature-new\" to be classified to \"New Features\"</p> <p>label \"pr/release/feature-changed\" to be classified to \"Changed Features\"</p> <p>label \"pr/release/feature-bug\" to be classified to \"Fixes\"</p> </li> <li> <p>build the chart package with the pushed tag, and submit a PR to branch 'github_pages' </p> <p>you cloud get the chart with command <code>helm repo add $REPO_NAME https://kdoctor-io.github.io/$REPO_NAME</code></p> </li> <li> <p>submit '/docs' to '/docs' of branch 'github_pages'</p> </li> <li> <p>create a GitHub Release attached with the chart package and changelog</p> </li> <li> <p>Finally, by hand, need approve the chart PR labeled as \"pr/release/robot_update_githubpage\" , and changelog PR labeled as \"pr/release/robot_update_githubpage\"</p> </li> </ol>"},{"location":"develop/release/#post","title":"post","text":"<ol> <li>Submit a issue of the version update to the documentation site --&gt; https://github.com/DaoCloud/DaoCloud-docs</li> </ol>"},{"location":"develop/roadmap/","title":"feature","text":"<p>\u6d4b\u8bd5\u96c6\u7fa4\u529f\u80fd\u3001\u6027\u80fd\u5de1\u68c0\u5de5\u5177\uff0c\u6279\u91cf\u5316\u5de1\u68c0\uff0c\u6781\u5927\u964d\u4f4e\u4eba\u5de5\u8d1f\u8f7d</p>"},{"location":"develop/roadmap/#crd","title":"CRD","text":""},{"location":"develop/roadmap/#netreachhealthy-and-httpapphealthy","title":"NetReachHealthy and HttpAppHealthy","text":"<ul> <li> <p>\u5927\u89c4\u6a21\u96c6\u7fa4\u90e8\u7f72\u540e\uff0c\u5de1\u68c0\u6bcf\u4e2a\u8282\u70b9\u4e0a\u7684\u7f51\u7edc\u60c5\u51b5\uff0c\u5177\u4f53\u662f\uff1a\u672c\u8282\u70b9\u662f\u5426\u80fd\u591f\u901a\u8fc7 pod ip\uff08multus\u591a\u7f51\u5361\uff09\u3001cluster ip\u3001   nodePort\u3001loadbalancer ip\u3001ingress ip \u3001\u591a\u7f51\u5361 \u7b49\u6240\u6709\u7f51\u7edc\u6e20\u9053\uff0c\u8bbf\u95ee\u5230\u96c6\u7fa4\u5176\u5b83\u8282\u70b9</p> </li> <li> <p>\u96c6\u7fa4\u6240\u6709\u7684 node \u4e0a \u53bb \u538b\u6d4b\u4e00\u4e2a\u96c6\u7fa4\u5185/\u5916\u7684\u5e94\u7528\u5730\u5740\uff0c\u4ee5\u67e5\u770b\u5e94\u7528\u7684\u6027\u80fd\u3001\u96c6\u7fa4\u6bcf\u4e2a\u89d2\u843d\u5230\u8fbe\u8be5\u5e94\u7528\u7684\u8fde\u901a\u6027\u3001\u7ed9\u5e94\u7528\u6ce8\u5165\u538b\u529b\u590d\u73b0\u67d0\u7c7bbug</p> </li> <li> <p>\u7ed9 api server \u6ce8\u5165\u538b\u529b\uff0c\u4ee5\u8f85\u52a9\u6392\u67e5 \u5176\u4ed6\u7ec4\u4ef6\uff08\u4f9d\u8d56 api server\uff09\u7684\u9ad8\u53ef\u7528</p> </li> <li> <p>\u751f\u4ea7\u548c\u5f00\u53d1\u73af\u5883\u7684\u5fc3\u8df3\u5de1\u68c0\uff0c\u4ee5qps=1\u4e3a\u538b\u529b\uff0c\u6bcf 1m \u95f4\u9694\u5de1\u68c0\u6574\u4e2a\u96c6\u7fa4\u5185 full mesh \u7f51\u7edc\u7684\u8fde\u901a\u6027\u3001\u5176\u5b83\u8282\u70b9\u5230\u4e00\u4e2a\u5e94\u7528\u7684\u53ef\u7528\u6027</p> </li> </ul>"},{"location":"develop/roadmap/#httpappdetect","title":"HttpAppDetect","text":"<ul> <li>\u81ea\u52a8\u53d1\u73b0\u5e94\u7528\u7684\u6700\u5927\u6027\u80fd</li> </ul>"},{"location":"develop/roadmap/#netdelayhealthy","title":"NetDelayHealthy","text":"<ul> <li>\u96c6\u7fa4\u8282\u70b9\u95f4\u7684\u5ef6\u65f6</li> </ul>"},{"location":"develop/roadmap/#dnshealthy","title":"DnsHealthy","text":"<ul> <li> <p>\u5927\u89c4\u6a21\u96c6\u7fa4\u90e8\u7f72\u540e\uff0c\u6d4b\u8bd5\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u89d2\u843d\u8bbf\u95ee dns \u7684\u8fde\u901a\u6027</p> </li> <li> <p>\u5927\u89c4\u6a21\u96c6\u7fa4\u90e8\u7f72\u540e\uff0c\u8c03\u8bd5 coredns \u7684\u526f\u672c\u6570\uff0c\u786e\u8ba4\u662f\u5426\u6ee1\u8db3\u8bbe\u8ba1\u9700\u6c42</p> </li> <li> <p>\u6d4b\u8bd5\u96c6\u7fa4\u5916\u90e8\u7684 DNS \u670d\u52a1</p> </li> </ul>"},{"location":"develop/roadmap/#dnsdetect","title":"DnsDetect","text":"<ul> <li>\u81ea\u52a8\u53d1\u73b0 dns \u7684\u6700\u5927\u6027\u80fd</li> </ul>"},{"location":"develop/roadmap/#nettcphealthy","title":"NetTcpHealthy","text":""},{"location":"develop/roadmap/#netudphealthy","title":"NetUdpHealthy","text":""},{"location":"develop/roadmap/#storagelocaldisk","title":"StorageLocalDisk","text":"<ul> <li>\u5927\u89c4\u6a21\u96c6\u7fa4\u90e8\u7f72\u540e\uff0c\u6d4b\u8bd5\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u7684\u78c1\u76d8 \u541e\u5410\u91cf \u548c \u5ef6\u65f6</li> </ul>"},{"location":"develop/roadmap/#cpupressure","title":"CpuPressure ?","text":"<ul> <li>\u7ed9\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u6ce8\u5165 CPU \u538b\u529b\uff0c\u4ee5\u6d4b\u8bd5\u5e94\u7528\u7684\u7a33\u5b9a\u6027\uff0c\u590d\u73b0\u4e00\u4e9b bug</li> </ul>"},{"location":"develop/roadmap/#memorypressure","title":"MemoryPressure ?","text":"<ul> <li>\u7ed9\u6bcf\u4e2a\u4e3b\u673a\u4e0a\u6ce8\u5165 memory \u538b\u529b\uff0c\u4ee5\u6d4b\u8bd5\u5e94\u7528\u7684\u7a33\u5b9a\u6027\uff0c\u590d\u73b0\u4e00\u4e9b bug</li> </ul>"},{"location":"develop/roadmap/#registryhealthy","title":"RegistryHealthy","text":"<ul> <li>\u68c0\u6d4b\u6bcf\u4e2a\u8282\u70b9\u5230\u955c\u50cf\u4ed3\u5e93\u7684\u8fde\u901a\u6027</li> </ul>"},{"location":"develop/roadmap/#k8sapihealthy","title":"K8sApiHealthy","text":""},{"location":"develop/roadmap/#mysqlhealthy","title":"MysqlHealthy","text":""},{"location":"develop/roadmap/#report","title":"report","text":"<p>\u652f\u6301\u901a\u8fc7 API \u83b7\u53d6\u62a5\u544a</p> <p>\u652f\u6301 pvc\u3001\u672c\u5730\u78c1\u76d8\u5b58\u50a8</p> <p>\u65e5\u5fd7\u5410\u51fa</p>"},{"location":"develop/roadmap/#metric","title":"metric","text":""},{"location":"develop/roadmap/#_1","title":"\u5176\u5b83","text":"<p>\u5982\u679c\u6709 job \u65f6\u95f4\u91cd\u53e0\u4e86\uff0c\u5219\u53ea\u5141\u8bb8\u8fd0\u884c\u4e00\u4e2a \u6216\u8005 \u591a\u4e2a\uff0c\u907f\u514d\u81ea\u8eab CPU \u4e0d\u8db3\u5f71\u54cd job \u7684\u7ed3\u679c</p> <p>\u4e2d\u95f4\u4ef6\u3001etcd \u7b49 \u63a2\u6d4b</p> kind feature status \u4efb\u52a1 \u652f\u6301\u5468\u671f\u548c\u4e00\u6b21\u6027\u8c03\u5ea6 \u591a\u4e2a\u4efb\u52a1\u5e76\u53d1\u65f6\uff0c\u652f\u6301\u89c4\u907f\uff0c\u907f\u514d cpu \u548c memory \u8017\u5c3d\uff0c\u4f7f\u5f97\u4efb\u52a1\u6267\u884c\u51c6\u786e \u6240\u6709\u4efb\u52a1\uff0c\u6700\u5927 qps \u5b89\u5168\u8fb9\u9645\u8bbe\u7f6e \u652f\u6301\u8bbe\u7f6e\u53d1\u538bqps\u3001\u53d1\u538b\u65f6\u95f4 \u652f\u6301\u8bbe\u7f6e\u5e76\u53d1\u538b\u6d4b worker \uff0c \u4e14\u652f\u6301\u8ddf\u968f\u5e94\u7528\u526f\u672c\u6570\u91cf\u81ea\u52a8\u5408\u9002\u5e76\u53d1\u6570\uff0c\u4ee5\u6ee1\u8db3K8S \u8d1f\u8f7d\u5747\u8861\u7279\u6027 \u652f\u6301select \u53d1\u538b pod \u7f51\u7edc\u53ef\u8fbe\u6027 \u652f\u6301 pod ip\uff0ccluster ip\uff0cnodePort\uff0cloadbalancer ip\uff0cingress\uff0c\u591a\u7f51\u5361\uff0cipv6 \u7b49\u591a\u6837\u5316\u6e20\u9053 \u6301\u7eed\u53d1\u538b\uff0c\u53d1\u73b0\u5076\u53d1\u4e22\u5305 \u670d\u52a1 http \u5de1\u68c0 \u652f\u6301 pod selector \u548c url \u6301\u7eed\u53d1\u538b\uff0c\u53d1\u73b0\u5076\u53d1\u4e22\u5305 \u652f\u6301 http/https/http2 \u652f\u6301\u5b9a\u5236 header\u3001method\u3001body dns\u5de1\u68c0 \u7f51\u7edc\u53ef\u8fbe\u6027 \u6027\u80fd\u6d4b\u8bd5 tcp \u5de1\u68c0 \u7f51\u7edc\u541e\u5410\u91cf udp \u5de1\u68c0 \u7f51\u7edc\u4e22\u5305\u7387 api server \u5de1\u68c0 \u7f51\u7edc\u53ef\u8fbe\u6027 \u5b58\u50a8\u5de1\u68c0 \u652f\u6301\u672c\u5730\u78c1\u76d8\u5de1\u68c0 IO \u541e\u5410\u91cf\u548c\u5ef6\u65f6 \u955c\u50cf\u4ed3\u5e93\u5de1\u68c0 \u7f51\u7edc\u53ef\u8fbe\u6027 \u4e2d\u95f4\u4ef6\u5de1\u68c0 \u7f51\u7edc\u53ef\u8fbe\u6027 mysql  redis \u62a5\u544a CR \u4e2d\u72b6\u6001\u5c55\u793a \u8be6\u7ec6\u62a5\u544a\u652f\u6301 pv \u5b58\u50a8 \u8be6\u7ec6\u62a5\u544a\u652f\u6301 API \u83b7\u53d6 \u8be6\u7ec6\u62a5\u544a\u652f\u6301 webhook \u5410\u51fa \u6307\u6807 \u8be6\u7ec6\u62a5\u544a\u7684\u4fdd\u7559\u65f6\u95f4\u8bbe\u7f6e \u62a5\u544a\u8f6e\u6eda\uff0c\u907f\u514d\u5b58\u6ee1 PVC"},{"location":"reference/arch/","title":"architecture","text":"<p>The kdoctor is aimed to test the cluster and generate task reports to check whether the cluster is healthy.</p> <p>It consists of controller deployment and agent daemonset.</p> <ul> <li> <p>the controller schedules the task, update and summarize the task result, and aggerate all reports.</p> </li> <li> <p>the agent implement tasks</p> </li> </ul>"},{"location":"reference/report/","title":"Report","text":""},{"location":"reference/report/#agent-report","title":"agent report","text":"<p>When agent finish task, it saves report to '/report' with name fmt.Sprintf(\"%s_%s_round%d_%s_%s\", kindName, taskName, roundNumber, nodeName, suffix). the report will be automatically deleted with age 'spec.schedulePlan.TimeoutMinute + 5 ' minutes. In this interval ,  the controller pod will collect this report and save to '/report' of controller pod</p>"},{"location":"reference/report/#controller-report","title":"controller report","text":"<p>when task finishes, it saves report to '/report' with name fmt.Sprintf(\"%s_%s_round%d_%s_%s\", kindName, taskName, roundNumber, nodeName, suffix). It also collects all agent report and  saves report to '/report'. All files in '/report' of controller will sevive with max age maxAgeInDay(default 30 days). It could be adjusted in the configmap</p> <p>the controller could save reports to host path or PVC</p>"},{"location":"usage/apphttphealthy/","title":"Nethttp","text":""},{"location":"usage/apphttphealthy/#concept","title":"concept","text":"<p>Fo this kind task, each kdoctor agent will send http request to specified target, and get success rate and mean delay.  It could specify success condition to tell the result succeed or fail.  And, more detailed report will print to kdoctor agent stdout, or save to disc by kdoctor controller.</p> <p>the following is the spec of nethttp</p> <pre><code>apiVersion: kdoctor.io/v1beta1\nkind: AppHttpHealthy\nmetadata:\n  name: httphealthy\nspec:\n  request:\n    durationInSecond: 10\n    perRequestTimeoutInMS: 1000\n    qps: 10\n  schedule:\n    roundNumber: 2\n    roundTimeoutMinute: 1\n    schedule: 1 1\n  expect:\n    meanAccessDelayInMs: 10000\n    successRate: 1\n    statusCode: 200\n  target:\n    bodyConfigmapName: http-body\n    bodyConfigmapNamespace: kube-system\n    header:\n    - Accept:text/html\n    host: https://10.6.172.20:9443\n    http2: false\n    method: PUT\n    tlsSecretName: https-cert\n    tlsSecretNamespace: kube-system\nstatus:\n  doneRound: 2\n  expectedRound: 2\n  finish: true\n  history:\n  - deadLineTimeStamp: \"2023-05-24T08:03:05Z\"\n    duration: 15.092667522s\n    endTimeStamp: \"2023-05-24T08:02:20Z\"\n    expectedActorNumber: 2\n    failedAgentNodeList: []\n    notReportAgentNodeList: []\n    roundNumber: 2\n    startTimeStamp: \"2023-05-24T08:02:05Z\"\n    status: succeed\n    succeedAgentNodeList:\n    - kdoctor-worker\n    - kdoctor-control-plane\n</code></pre> <ul> <li> <p>spec.schedule: set how to schedule the task.</p> <p>roundNumber: how many rounds it should be to run this task</p> <p>schedule: Support Linux crontab syntax for scheduling tasks, while also supporting simple writing.              The first digit represents how long the task will start, and the second digit represents the interval time between each round of tasks,             separated by spaces. Example: \"1 2\" indicates that the task will start in 1 minute, and the interval time between each round of tasks.</p> <p>roundTimeoutMinute: the timeout in minute for each round, when the rask does not finish in time, it results to be failuire</p> <p>sourceAgentNodeSelector [optional]: set the node label selector, then, the kdoctor agent who locates on these nodes will implement the task. If not set this field, all kdoctor agent will execute the task</p> </li> <li> <p>spec.request: how each kdoctor agent should send the http request</p> <p>durationInSecond: for each round, the duration in second how long the http request lasts</p> <p>perRequestTimeoutInMS: timeout in ms for each http request </p> <p>qps: qps</p> </li> <li> <p>spec.target: set the target of http request.</p> <pre><code>host: the host for http, example service ip, pod ip, service domain, an user-defined UR\n\nmethod: http method, must be one of GET POST PUT DELETE CONNECT OPTIONS PATCH HEAD\n\nbodyConfigmapName: The body configmap name\n</code></pre> <ul> <li> <p>bodyConfigmapNamespace: The body configmap namespace </p> <p>tlsSecretName: The tls secret name *  tlsSecretNamespace: The tls secret namespace</p> <p>header:  HTTP request header</p> <p>http2: Requests are made using the http2 protocol</p> <p>notice: when test targetAgent case, it will send http request to all targets at the same time with spec.request.qps for each one. That meaning, the actually QPS may be bigger than spec.request.qps</p> </li> </ul> </li> <li> <p>spec.expect: define the success condition of the task result </p> <p>meanAccessDelayInMs: mean access delay in MS, if the actual delay is bigger than this, it results to be failure</p> <p>successRate: the success rate of all http requests. Notice, when a http response code is &gt;=200 and &lt; 400, it's treated as success. if the actual whole success rate is smaller than successRate, the task results to be failure</p> <p>statusCode: Expect the HTTP status code returned by each request </p> </li> <li> <p>status: the status of the task</p> <p>doneRound: how many rounds have finished</p> <p>expectedRound: how many rounds the task expect</p> <p>finish: whether all rounds of this task have finished</p> <p>lastRoundStatus: the result of last round</p> <p>history:</p> <pre><code>roundNumber: the round number\n\nstatus: the status of this round\n\nstartTimeStamp: when this round begins\n\nendTimeStamp: when this round finally finished\n\nduration: how long the round spent\n\ndeadLineTimeStamp: the time deadline of a round\n\nfailedAgentNodeList: the node list where failed kdoctor agent locate\n\nnotReportAgentNodeList: the node list where uknown kdoctor agent locate. This means these agents have problems.\n\nsucceedAgentNodeList: the node list where successful kdoctor agent locate\n</code></pre> </li> </ul>"},{"location":"usage/apphttphealthy/#example","title":"example","text":"<p>a quick task to test kdoctor agent, to verify the whole network is ok, each agent could reach specific host</p> <pre><code>\ncat &lt;&lt;EOF &gt; test-httpapphealthy.yaml\napiVersion: kdoctor.io/v1beta1\nkind: AppHttpHealthy\nmetadata:\n  name: httphealthy\nspec:\n  request:\n    durationInSecond: 10\n    perRequestTimeoutInMS: 1000\n    qps: 10\n  schedule:\n    roundNumber: 2\n    roundTimeoutMinute: 1\n    schedule: 1 1\n  expect:\n    meanAccessDelayInMs: 10000\n    successRate: 1\n    statusCode: 200\n  target:\n    bodyConfigmapName: http-body\n    bodyConfigmapNamespace: kube-system\n    header:\n    - Accept:text/html\n    host: https://10.6.172.20:9443\n    http2: false\n    method: PUT\n    tlsSecretName: https-cert\n    tlsSecretNamespace: kube-system\nEOF\nkubectl apply -f test-httpapphealthy.yaml\n\n</code></pre>"},{"location":"usage/apphttphealthy/#example-body","title":"example body","text":"<pre><code>cat &lt;&lt;EOF &gt; http-body.yaml\napiVersion: v1\ndata:\n  body: |\n    {test:test}\nkind: ConfigMap\nmetadata:\n  name: http-body\n  namespace: kube-system\nEOF\nkubectl apply -f http-body.yaml\n</code></pre>"},{"location":"usage/apphttphealthy/#example-https-cert","title":"example https cert","text":"<pre><code>cat &lt;&lt;EOF &gt; https-cert.yaml\napiVersion: v1\ndata:\n  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURWekNDQWorZ0F3SUJBZ0lKQU1vL3p5bGZZZzVSTUEwR0NTcUdTSWIzRFFFQkN3VUFNRUl4Q3pBSkJnTlYKQkFZVEFsaFlNUlV3RXdZRFZRUUhEQXhFWldaaGRXeDBJRU5wZEhreEhEQWFCZ05WQkFvTUUwUmxabUYxYkhRZwpRMjl0Y0dGdWVTQk1kR1F3SGhjTk1qTXdOakE0TURrd05URTVXaGNOTWpRd05qQTNNRGt3TlRFNVdqQkNNUXN3CkNRWURWUVFHRXdKWVdERVZNQk1HQTFVRUJ3d01SR1ZtWVhWc2RDQkRhWFI1TVJ3d0dnWURWUVFLREJORVpXWmgKZFd4MElFTnZiWEJoYm5rZ1RIUmtNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQQpwWERBUGt1UzhvNW9lRTBBS0ZxL2Vjb1pjN2hFbXk1RlMvbWxlZCt2MFRFMlV5cGord1k0M0hvaEhpWjl3bVRECkpwTHZTKzgwTFpmMitrVkNBb05hTjdMdU1rdFJKaXlQUDc2TklSaUdPdzl6MmZpNHNIaUFnS0dvR1ZMb1c1YUMKa0RoK3dLKzh5NDVnVGZGR1VaWGpBa0pKSm1mWDd2TXllbkpyT2J5SUE0ajFuc294cDBNelFzNkYzREQ1TmdmZApQc1JtT3N6QlNLRTdNaFF4MEN5RlVQWjRTZ3U2N25MQytmRFBWVXBYY3pKQU1ZSTVrNWpyaElneDZnR2hKVFk0CmRLQ0VMWllwUmhCMWFFbVBIRjFlVUZ3MC9FcG5ldUdPd1ZqazZsSEp6QUxRUHBnR1dBZ0V1WFFVckxYb0dNclAKcWJrYU9WeitMelh0N1ZCaWJOZmdFUUlEQVFBQm8xQXdUakFkQmdOVkhRNEVGZ1FVTDlnL3FhZ2ptaGJ1K1pvQQpRVkFOdE1nd0cra3dId1lEVlIwakJCZ3dGb0FVTDlnL3FhZ2ptaGJ1K1pvQVFWQU50TWd3Rytrd0RBWURWUjBUCkJBVXdBd0VCL3pBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQW5Jd0Jyc3paY0pRRGFrZnNVVHp1eEtORmdUNUoKNVJOUWQ1S1NZM01HTWVkQjNIU2dMTEVWM3FTM1pLNHlrT3NFaXJ6c1lmdGV2Q1JGL2VsTkVIZDREQ2tiRzlSeQpwTzAwYTk1RkdFNktUWk1iSTRDaW1kMmF6eVhkazMyYUtzeFpjaDBRMzlneHgwSGFnVW5CcDk1VWxaQkcyOTh2CnhkQVRtSXZJNmVpd2FCeWc2NjBKRklRZ1VkVmhUM0VNTUI2dUlxaWdKZTJlMEcvV0t1My9BNGhvL3hDZUVNWmYKbFJlRXJIeFo0TzZsVTVEM0pnNURWcUM1MlBPK0V2aTJpdm5ZTmNvbTRibU9HbE41RmhzdG5La3M2ZlVsV012RQpXeXZCb01OU2VFNllueVFUUURBZ09BN0NlVzhKSUk4b3JRN00vM0JnWlNSOUZ4OGdhY01jeEUydTZRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\n  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURBRENDQWVnQ0NRRGljN284NjltaW16QU5CZ2txaGtpRzl3MEJBUXNGQURCQ01Rc3dDUVlEVlFRR0V3SlkKV0RFVk1CTUdBMVVFQnd3TVJHVm1ZWFZzZENCRGFYUjVNUnd3R2dZRFZRUUtEQk5FWldaaGRXeDBJRU52YlhCaApibmtnVEhSa01CNFhEVEl6TURZd09EQTVNak13TlZvWERUSTBNRFl3TnpBNU1qTXdOVm93UWpFTE1Ba0dBMVVFCkJoTUNXRmd4RlRBVEJnTlZCQWNNREVSbFptRjFiSFFnUTJsMGVURWNNQm9HQTFVRUNnd1RSR1ZtWVhWc2RDQkQKYjIxd1lXNTVJRXgwWkRDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTm5RUnVVaApnbWhtdFV4aitJQXpIayswdU5Oa2ZnUWFVMHpYdG1ZMGI3T0g3VkZXd2tVWDNJYVU2Q2JJRzd2MVRPbllISmZhCloyZmRvNURudlpaSk5OWnZOWUdEbkU4d3JuNmVHSlFpMW5hbDlEbGdyaWJPSWJtOUU4N055VUFTRXpkSEFhN1oKOUp1bEVlRlE5VXJHV25KbXhEZjBTSGM4ckJGRnBsMVpORXpUbDFpUFFhaDRDVGcya1lWMWJtS0h2cmhUcDV6UQp4VE4wT0ZUa1JKWmprN284YTRxUXhBVWYvU1ZkQ3BkeXBHYzhPM3JWL0dPMTdneFlVK2lmRmY0OW84M2l5ZGluCm0zQ3NmOFRFVWlNTGZqcDgwQk5ZQTVScVJPSG15NTVNSG9TM2VnWEJWOG5va3hvVHkzSy8rdnd0L21BMmVLQWEKVHRXeEo1NVM2T0M5R0xrQ0F3RUFBVEFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBalYrWWpKSkU1c2o5ODZqQgpzenY3cXhkdXRGSHdIM1NpRXhzaEt0S3VpaE1BYjBJR1V4MEIxbmdTZ1gvNUVqUzEwYTZtRmhJZGxWS29PSDJ1CkQ3aVZtRkdweHBWaUtCTFMwVnhweGphVmZ0OExCd2k1cHN5eDZyWmEwaFMvek1NMEFlL1FuQXpoSzZDMDl5T08KN1g1R2orMjNQQjBVNkorZnRteThMYVpwK0ZBbWFobi9OYThJbmJNY1hEQjVEeEhNUWkzdjFrQUh6bnBGNU02KwpuSEkyR3B0RzR4UzlDTitFK2FBa3NBZGMzY2VjZ3JCL04vUFZNMWhFdkZtakw5SVpTdEJkYzBGQ1pGMHJPVy96CllhWVhLa3FRTm9Wa2FaMENsRVEybWdIMFh0ZktzQ0VZVGx0OGJncDgveDdKTmlqN2UvYkoyU0E0M015NTF1ZHYKZ0N5M1pRPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\n  tls.key: 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\nkind: Secret\nmetadata:\n  name: https-cert\n  namespace: kube-system\ntype: kubernetes.io/tls\nEOF\nkubectl apply -f https-cert.yaml\n</code></pre> <p>notice: key body ca.crt tls.crt and tls.key are fixed field cannot be customized</p>"},{"location":"usage/apphttphealthy/#debug","title":"debug","text":"<p>when something wrong happen, see the log for your task with following command</p> <pre><code>#get log \nCRD_KIND=\"apphttphealthy\"\nCRD_NAME=\"httphealthy\"\nkubectl logs -n kube-system  kdoctor-agent-v4vzx | grep -i \"${CRD_KIND}.${CRD_NAME}\"\n\n</code></pre>"},{"location":"usage/apphttphealthy/#report","title":"report","text":"<p>when the kdoctor is not enabled to aggerate reports, all reports will be printed in the stdout of kdoctor agent. Use the following command to get its report</p> <pre><code>kubectl logs -n kube-system  kdoctor-agent-v4vzx | jq 'select( .TaskName==\"apphttphealthy.httphealthy\" )'\n</code></pre> <p>when the kdoctor is enabled to aggregate reports, all reports will be collected in the PVC or hostPath of kdoctor controller.</p> <p>metric introduction</p> <pre><code>        \"FailureReason\": \"\",\n        \"MeanDelay\": 34.36,\n        \"Metrics\": {\n            \"start\": \"2023-05-24T09:00:35.03987095Z\",\n            \"end\": \"2023-05-24T09:00:45.08774646Z\",\n            \"duration\": \"10.04787551s\",\n            \"requestCount\": 100,\n            \"successCount\": 100,\n            \"tps\": 9.952352604336754,\n            \"total_request_data\": \"23247 byte\",\n            \"latencies\": {\n                \"P50_inMs\": 35,\n                \"P90_inMs\": 57,\n                \"P95_inMs\": 58,\n                \"P99_inMs\": 59,\n                \"Max_inMx\": 62,\n                \"Min_inMs\": 18,\n                \"Mean_inMs\": 34.36\n            },\n            \"status_codes\": {\n                \"200\": 100\n            },\n            \"errors\": {}\n        },\n        \"Succeed\": \"true\",\n        \"SucceedRate\": \"1\",\n        \"TargetMethod\": \"GET\",\n        \"TargetName\": \"AppHttpHealthy target\",\n        \"TargetNumber\": \"1\",\n        \"TargetType\": \"AppHttpHealthy\",\n        \"TargetUrl\": \"http://kdoctor-agent-ipv4.kube-system.svc.cluster.local\"\n</code></pre>"},{"location":"usage/install/","title":"install","text":""},{"location":"usage/install/#production-env","title":"production env","text":""},{"location":"usage/install/#poc-or-e2e-environment","title":"POC or E2E environment","text":"<p>when POC or E2E case, it could disable the controller to collect reports, so no need to install strogeClass.</p> <p>the following method leads the agent just print report to console</p> <pre><code>helm repo add kdoctor https://kdoctor-io.github.io/kdoctor\n\nhelm install kdoctor kdoctor/kdoctor \\\n    -n kube-system --wait --debug \\\n    --set feature.enableIPv4=true --set feature.enableIPv6=true \\\n    --set feature.aggregateReport.enabled=false\n</code></pre> <p>the following method leads controller collects all report to disc of local host. BTW, when the kdoctor controller is schedules to other nodes, the historical reports will be not migrated </p> <pre><code>helm repo add kdoctor https://kdoctor-io.github.io/kdoctor\n\nhelm  install kdoctor kdoctor/kdoctor \\\n    -n kube-system --wait --debug \\\n    --set feature.enableIPv4=true --set feature.enableIPv6=true \\\n    --set feature.aggregateReport.enabled=true \\\n    --set feature.aggregateReport.controller.reportHostPath=\"/var/run/kdoctor/controller\"\n</code></pre>"},{"location":"usage/install/#production-environment","title":"production environment","text":"<p>the following method leads the kdoctor controller collect report to stroage, so firstly, it should install storageClass</p> <pre><code>helm repo add kdoctor https://kdoctor-io.github.io/kdoctor\n\nhelm  install kdoctor kdoctor/kdoctor \\\n    -n kube-system --wait --debug \\\n    --set feature.enableIPv4=true --set feature.enableIPv6=true \\\n    --set feature.aggregateReport.enabled=true \\\n    --set feature.aggregateReport.controller.pvc.enabled=true \\\n    --set feature.aggregateReport.controller.pvc.storageClass=local \\\n    --set feature.aggregateReport.controller.pvc.storageRequests=\"100Mi\" \\\n    --set feature.aggregateReport.controller.pvc.storageLimits=\"500Mi\"\n</code></pre>"},{"location":"usage/install/#multus-environment","title":"multus environment","text":"<p>if it is required to test all interface of agent pod, it should annotate the agent with multus annotation</p> <pre><code>helm repo add kdoctor https://kdoctor-io.github.io/kdoctor\n\n# replace following with actual multus configuration\nMULTUS_DEFAULT_CNI=kube-system/k8s-pod-network\nMULTUS_ADDITIONAL_CNI=kube-system/macvlan\n\nhelm install kdoctor kdoctor/kdoctor \\\n    -n kube-system --wait --debug \\\n    --set feature.enableIPv4=true --set feature.enableIPv6=true \\\n    --set feature.aggregateReport.enabled=false \\\n    --set kdoctorAgent.podAnnotations.v1\\.multus-cni\\.io/default-network=${MULTUS_DEFAULT_CNI} \\\n    --set kdoctorAgent.podAnnotations.k8s\\.v1\\.cni\\.cncf\\.io/networks=${MULTUS_ADDITIONAL_CNI}\n\n</code></pre>"},{"location":"usage/netdns/","title":"netdns","text":""},{"location":"usage/netdns/#concept","title":"concept","text":"<p>Fo this kind task, each kdoctor agent will send dns request to specified target, and get success rate and mean delay. It could specify success condition to tell the result succeed or fail. And, more detailed report will print to kdoctor agent stdout, or save to disc by kdoctor controller.</p> <p>the following is the spec of netdns</p> <pre><code>\ncat &lt;&lt;EOF &gt; netdns.yaml\napiVersion: kdoctor.io/v1beta1\nkind: Netdns\nmetadata:\n  name: testdns\nspec:\n  schedule:\n    schedule: \"1 1\"\n    roundNumber: 2\n    roundTimeoutMinute: 1\n  target:\n    targetDns:\n      testIPv4: true\n      testIPv6: false\n      serviceName: coredns\n      serviceNamespace: kube-system\n    targetUser:\n      server: 172.18.0.1\n      port: 53\n  request:\n    durationInSecond: 10\n    qps: 20\n    perRequestTimeoutInMS: 500\n    domain: \"kube-dns.kube-system.svc.cluster.local\"\n    protocol: udp\n  expect:\n    successRate: 1\n    meanAccessDelayInMs: 10000\nEOF\n\nkubectl apply -f netdns.yaml\n\n</code></pre> <ul> <li> <p>spec.schedule: set how to schedule the task.</p> <p>roundNumber: how many rounds it should be to run this task</p> <p>schedule: Support Linux crontab syntax for scheduling tasks, while also supporting simple writing.              The first digit represents how long the task will start, and the second digit represents the interval time between each round of tasks,             separated by spaces. Example: \"1 2\" indicates that the task will start in 1 minute, and the interval time between each round of tasks.</p> <p>roundTimeoutMinute: the timeout in minute for each round, when the rask does not finish in time, it results to be failuire</p> <p>sourceAgentNodeSelector [optional]: set the node label selector, then, the kdoctor agent who locates on these nodes will implement the task. If not set this field, all kdoctor agent will execute the task</p> </li> <li> <p>spec.request: how each kdoctor agent should send the dns request</p> <p>durationInSecond: for each round, the duration in second how long the dns request lasts</p> <p>perRequestTimeoutInMS: timeout in ms for each dns request</p> <p>qps: qps</p> <p>domain\uff1a resolved domain</p> </li> <li> <p>spec.target: set the target of dns request. it could not set targetUser and targetDns at the same time</p> <p>targetUser [optional]: set an user-defined DNS server for the dns request</p> <pre><code>server: the address for dns server\n\nport: the port for dns server\n</code></pre> <p>targetDns: [optional]: set cluster dns server for the dns request</p> <pre><code>testIPv4: test DNS server IPv4 address and request is type A.\n\ntestIPv6: test DNS server IPv6 address and request is type AAAA.\n\nserviceName: Specify the name of the DNS to be tested\n</code></pre> <ul> <li> <p>serviceNamespace: Specify the namespace of the DNS to be tested</p> </li> </ul> <p>protocol: Specify request protocol,Optional value udp\uff0ctcp\uff0ctcp-tls,default udp.</p> </li> <li> <p>spec.expect: define the success condition of the task result</p> </li> </ul> <p>meanAccessDelayInMs: mean access delay in MS, if the actual delay is bigger than this, it results to be failure</p> <p>successRate: the success rate of all dns requests. Notice, when a dns response code is &gt;=200 and &lt; 400, it's treated as success. if the actual whole success rate is smaller than successRate, the task results to be failure</p> <ul> <li>status: the status of the task   doneRound: how many rounds have finished</li> </ul> <p>expectedRound: how many rounds the task expect</p> <p>finish: whether all rounds of this task have finished</p> <p>lastRoundStatus: the result of last round</p> <p>history:   roundNumber: the round number</p> <pre><code>    status: the status of this round\n\n    startTimeStamp: when this round begins\n\n    endTimeStamp: when this round finally finished\n\n    duration: how long the round spent\n\n    deadLineTimeStamp: the time deadline of a round\n\n    failedAgentNodeList: the node list where failed kdoctor agent locate\n\n    notReportAgentNodeList: the node list where uknown kdoctor agent locate. This means these agents have problems.\n\n    succeedAgentNodeList: the node list where successful kdoctor agent locate\n</code></pre>"},{"location":"usage/netdns/#example","title":"example","text":"<p>test custom dns server by crontab</p> <pre><code>\ncat &lt;&lt;EOF &gt; netdns1.yaml\napiVersion: kdoctor.io/v1beta1\nkind: Netdns\nmetadata:\n  name: testdns\nspec:\n  schedule:\n    schedule: \"*/1 * * * *\"\n    roundNumber: 2\n    roundTimeoutMinute: 1\n  target:\n    targetUser:\n      server: 172.18.0.1\n      port: 53\n  request:\n    durationInSecond: 10\n    qps: 10\n    perRequestTimeoutInMS: 500\n    domain: \"baidu.com\"\n    protocol: udp\n  expect:\n    successRate: 1\n    meanAccessDelayInMs: 1000\nEOF\n\nkubectl apply -f netdns1.yaml\n\n</code></pre> <p>test custom dns server by simple</p> <pre><code>\ncat &lt;&lt;EOF &gt; netdns1.yaml\napiVersion: kdoctor.io/v1beta1\nkind: Netdns\nmetadata:\n  name: testdns\nspec:\n  schedule:\n    schedule: \"1 1\"\n    roundNumber: 2\n    roundTimeoutMinute: 1\n  target:\n    protocol: udp\n    targetUser:\n      server: 172.18.0.1\n      port: 53\n  request:\n    durationInSecond: 10\n    qps: 10\n    perRequestTimeoutInMS: 500\n    domain: \"baidu.com\"\n  expect:\n    successRate: 1\n    meanAccessDelayInMs: 1000\nEOF\n\nkubectl apply -f netdns1.yaml\n\n</code></pre> <p>test cluster dns server by crontab</p> <pre><code>\ncat &lt;&lt;EOF &gt; netdns.yaml\napiVersion: kdoctor.io/v1beta1\nkind: Netdns\nmetadata:\n  name: testdns\nspec:\n  schedule:\n    schedule: \"*/1 * * * *\"\n    roundNumber: 2\n    roundTimeoutMinute: 1\n  target:\n    targetDns:\n      testIPv4: true\n      testIPv6: false\n      serviceNamespaceName: kube-system/kube-dns\n    protocol: udp\n  request:\n    durationInSecond: 10\n    qps: 20\n    perRequestTimeoutInMS: 500\n    domain: \"kube-dns.kube-system.svc.cluster.local\"\n  expect:\n    successRate: 1\n    meanAccessDelayInMs: 10000\nEOF\n\nkubectl apply -f netdns.yaml\n\n</code></pre> <p>test cluster dns server by simple</p> <pre><code>\ncat &lt;&lt;EOF &gt; netdns.yaml\napiVersion: kdoctor.io/v1beta1\nkind: Netdns\nmetadata:\n  name: testdns\nspec:\n  schedule:\n    schedule: \"1 1\"\n    roundNumber: 2\n    roundTimeoutMinute: 1\n  target:\n    targetDns:\n      testIPv4: true\n      testIPv6: false\n      serviceNamespaceName: kube-system/test-app\n    protocol: udp\n  request:\n    durationInSecond: 10\n    qps: 20\n    perRequestTimeoutInMS: 500\n    domain: \"kube-dns.kube-system.svc.cluster.local\"\n  expect:\n    successRate: 1\n    meanAccessDelayInMs: 10000\nEOF\n\nkubectl apply -f netdns.yaml\n\n</code></pre>"},{"location":"usage/netdns/#report","title":"report","text":"<p>when the kdoctor is not enabled to aggerate reports, all reports will be printed in the stdout of kdoctor agent. Use the following command to get its report</p> <pre><code>kubectl logs -n kube-system  kdoctor-agent-lwhtm | jq 'select( .TaskName==\"netdns.testdns\" )'\n</code></pre> <p>when the kdoctor is enabled to aggregate reports, all reports will be collected in the PVC or hostPath of kdoctor controller.</p> <p>metric introduction</p> <pre><code>{\n  \"TaskName\": \"netdns.testdns\",\n  \"TaskSpec\": {\n    \"schedule\": {\n      \"schedule\": \"1 1\",\n      \"roundTimeoutMinute\": 1,\n      \"roundNumber\": 2\n    },\n    \"target\": {\n      \"protocol\": \"tcp\"\n    },\n    \"request\": {\n      \"durationInSecond\": 10,\n      \"qps\": 20,\n      \"perRequestTimeoutInMS\": 500,\n      \"domain\": \"kube-dns.kube-system.svc.cluster.local\"\n    },\n    \"success\": {\n      \"successRate\": 1,\n      \"meanAccessDelayInMs\": 10000\n    }\n  },\n  \"RoundNumber\": 1,\n  \"RoundResult\": \"succeed\",\n  \"NodeName\": \"kdoctor-control-plane\",\n  \"PodName\": \"kdoctor-agent-lwhtm\",\n  \"FailedReason\": \"\",\n  \"StartTimeStamp\": \"2023-04-27T07:07:32.032814878Z\",\n  \"EndTimeStamp\": \"2023-04-27T07:07:32.070513569Z\",\n  \"RoundDuraiton\": \"37.69869ms\",\n  \"ReportType\": \"agent test report\",\n  \"Detail\": {}\n}\n</code></pre>"},{"location":"usage/netreach/","title":"Nethttp","text":""},{"location":"usage/netreach/#concept","title":"concept","text":"<p>Fo this kind task, each kdoctor agent will send http request to specified target, and get success rate and mean delay.  It could specify success condition to tell the result succeed or fail.  And, more detailed report will print to kdoctor agent stdout, or save to disc by kdoctor controller.</p> <p>the following is the spec of nethttp</p> <pre><code>apiVersion: kdoctor.io/v1beta1\nkind: NetReach\nmetadata:\n  creationTimestamp: \"2023-05-24T08:11:13Z\"\n  generation: 1\n  name: netreach\n  resourceVersion: \"1427617\"\n  uid: 2f5da0d6-0252-4229-adca-a43a5d2ac4ff\nspec:\n  request:\n    durationInSecond: 10\n    perRequestTimeoutInMS: 1000\n    qps: 10\n  schedule:\n    roundNumber: 2\n    roundTimeoutMinute: 1\n    schedule: 1 1\n  expect:\n    meanAccessDelayInMs: 10000\n    successRate: 1\n  target:\n    clusterIP: true\n    endpoint: true\n    ingress: false\n    ipv4: true\n    ipv6: true\n    loadBalancer: false\n    multusInterface: false\n    nodePort: true\nstatus:\n  doneRound: 2\n  expectedRound: 2\n  finish: true\n  history:\n  - deadLineTimeStamp: \"2023-05-24T08:14:13Z\"\n    duration: 20.089468009s\n    endTimeStamp: \"2023-05-24T08:13:33Z\"\n    expectedActorNumber: 2\n    failedAgentNodeList: []\n    notReportAgentNodeList: []\n    roundNumber: 2\n    startTimeStamp: \"2023-05-24T08:13:13Z\"\n    status: succeed\n    succeedAgentNodeList:\n    - kdoctor-worker\n    - kdoctor-control-plane\n</code></pre> <ul> <li> <p>spec.schedule: set how to schedule the task.</p> <p>roundNumber: how many rounds it should be to run this task</p> <p>schedule: Support Linux crontab syntax for scheduling tasks, while also supporting simple writing.              The first digit represents how long the task will start, and the second digit represents the interval time between each round of tasks,             separated by spaces. Example: \"1 2\" indicates that the task will start in 1 minute, and the interval time between each round of tasks.</p> <p>roundTimeoutMinute: the timeout in minute for each round, when the rask does not finish in time, it results to be failuire</p> <p>sourceAgentNodeSelector [optional]: set the node label selector, then, the kdoctor agent who locates on these nodes will implement the task. If not set this field, all kdoctor agent will execute the task</p> </li> <li> <p>spec.request: how each kdoctor agent should send the http request</p> <p>durationInSecond: for each round, the duration in second how long the http request lasts</p> <p>perRequestTimeoutInMS: timeout in ms for each http request </p> <p>qps: qps</p> </li> <li> <p>spec.target: set the target of http request. it could not set targetUser and targetAgent at the same time</p> <p>target: [optional]: set the http tareget to kdoctor agents</p> <pre><code>clusterIP: send http request to the cluster ipv4 or ipv6 address of kdoctor agnent, according to ipv4 and ipv6.\n\nendpoint: send http request to other kdoctor agnent ipv4 or ipv6 address according to testIPv4 and testIPv6.\n\nmultusInterface: whether send http request to all interfaces ip in testEndpoint case.\n\nipv4: test any IPv4 address. Notice, the 'enableIPv4' in configmap  spiderdocter must be enabled\n\nipv6: test any IPv6 address. Notice, the 'enableIPv6' in configmap  spiderdocter must be enabled\n\ningress: send http request to the ingress ipv4 or ipv6 address of kdoctor agnent\n\nnodePort: send http request to the nodePort ipv4 or ipv6 address with each local node of kdoctor agnent , according to testIPv4 and testIPv6.\n\n&gt;notice: when test targetAgent case, it will send http request to all targets at the same time with spec.request.qps for each one. That meaning, the actually QPS may be bigger than spec.request.qps\n</code></pre> </li> <li> <p>spec.expect: define the success condition of the task result </p> <p>meanAccessDelayInMs: mean access delay in MS, if the actual delay is bigger than this, it results to be failure</p> <p>successRate: the success rate of all http requests. Notice, when a http response code is &gt;=200 and &lt; 400, it's treated as success. if the actual whole success rate is smaller than successRate, the task results to be failure</p> </li> <li> <p>status: the status of the task     doneRound: how many rounds have finished</p> <p>expectedRound: how many rounds the task expect</p> <p>finish: whether all rounds of this task have finished</p> <p>lastRoundStatus: the result of last round</p> <p>history:     roundNumber: the round number</p> <pre><code>status: the status of this round\n\nstartTimeStamp: when this round begins\n\nendTimeStamp: when this round finally finished\n\nduration: how long the round spent\n\ndeadLineTimeStamp: the time deadline of a round\n\nfailedAgentNodeList: the node list where failed kdoctor agent locate\n\nnotReportAgentNodeList: the node list where uknown kdoctor agent locate. This means these agents have problems.\n\nsucceedAgentNodeList: the node list where successful kdoctor agent locate\n</code></pre> </li> </ul>"},{"location":"usage/netreach/#example","title":"example","text":"<p>a quick task to test kdoctor agent, to verify the whole network is ok, each agent could reach each other</p> <pre><code>\ncat &lt;&lt;EOF &gt; netreachhealthy-test-agent.yaml\napiVersion: kdoctor.io/v1beta1\nkind: NetReach\nmetadata:\n  generation: 1\n  name: netreach\nspec:\n  request:\n    durationInSecond: 10\n    perRequestTimeoutInMS: 1000\n    qps: 10\n  schedule:\n    roundNumber: 2\n    roundTimeoutMinute: 1\n    schedule: 1 1\n  expect:\n    meanAccessDelayInMs: 10000\n    successRate: 1\n  target:\n    clusterIP: true\n    endpoint: true\n    ingress: false\n    ipv4: true\n    ipv6: true\n    loadBalancer: false\n    multusInterface: false\n    nodePort: true\nEOF\nkubectl apply -f netreachhealthy-test-agent.yaml\n\n</code></pre>"},{"location":"usage/netreach/#debug","title":"debug","text":"<p>when something wrong happen, see the log for your task with following command</p> <pre><code>#get log \nCRD_KIND=\"netreachhealthy\"\nCRD_NAME=\"netreach\"\nkubectl logs -n kube-system  kdoctor-agent-v4vzx | grep -i \"${CRD_KIND}.${CRD_NAME}\"\n\n</code></pre>"},{"location":"usage/netreach/#report","title":"report","text":"<p>when the kdoctor is not enabled to aggerate reports, all reports will be printed in the stdout of kdoctor agent. Use the following command to get its report</p> <pre><code>kubectl logs -n kube-system  kdoctor-agent-v4vzx | jq 'select( .TaskName==\"netreachhealthy.netreach\" )'\n</code></pre> <p>when the kdoctor is enabled to aggregate reports, all reports will be collected in the PVC or hostPath of kdoctor controller.</p> <p>metric introduction</p> <pre><code>      {\n        \"FailureReason\": \"\",\n        \"MeanDelay\": 106.84,\n        \"Metrics\": {\n          \"start\": \"2023-05-24T08:13:13.530015031Z\",\n          \"end\": \"2023-05-24T08:13:28.560982373Z\",\n          \"duration\": \"15.030967342s\",\n          \"requestCount\": 150,\n          \"successCount\": 150,\n          \"tps\": 9.979397638691244,\n          \"total_request_data\": \"34866 byte\",\n          \"latencies\": {\n            \"P50_inMs\": 103,\n            \"P90_inMs\": 199,\n            \"P95_inMs\": 202,\n            \"P99_inMs\": 204,\n            \"Max_inMx\": 205,\n            \"Min_inMs\": 3,\n            \"Mean_inMs\": 106.84\n          },\n          \"status_codes\": {\n            \"200\": 150\n          },\n          \"errors\": {}\n        },\n        \"Succeed\": \"true\",\n        \"SucceedRate\": \"1\",\n        \"TargetMethod\": \"GET\",\n        \"TargetName\": \"AgentClusterV4IP_172.41.156.187:80\",\n        \"TargetUrl\": \"http://172.41.156.187:80\"\n      }\n</code></pre>"},{"location":"usage/performance/","title":"environment","text":"<ul> <li>Kubenetes: <code>v1.25.4</code></li> <li>container runtime: <code>containerd 1.6.12</code></li> <li>OS: <code>CentOS Linux 8</code></li> <li>kernel: <code>4.18.0-348.7.1.el8_5.x86_64</code></li> </ul> Node Role CPU Memory master1 control-plane 4C 8Gi master2 control-plane 4C 8Gi master3 control-plane 4C 8Gi worker4 3C 8Gi worker5 3C 8Gi worker6 3C 8Gi worker7 3C 8Gi worker8 3C 8Gi worker9 3C 8Gi worker10 3C 8Gi"},{"location":"usage/performance/#nethttp","title":"Nethttp","text":"<p>In a pod with a CPU of 1 </p> <p>The test server is a server that sleeps for one second and then returns</p>"},{"location":"usage/performance/#http11","title":"Http1.1","text":"client time requests qps Memory kdoctor 0.5m 89660 2988.67 210Mb ab 0.5m 76700 2599.31 60Mb wrk 0.5m 86105 2867.67 50Mb hey 0.5m 58423 1947.42 210Mb client time requests qps Memory kdoctor 1m 179634 2993.9 210Mb ab 1m 153875 2564.59 60Mb wrk 1m 176966 2945.69 50Mb hey 1m 118452 1974.2 220Mb client time requests qps Memory kdoctor 5m 897979 2993.26 210Mb ab 5m 763983 2546.61 60Mb wrk 5m 895324 2983.71 50Mb hey 5m 596077 1986.92 270Mb"},{"location":"usage/performance/#http2","title":"Http2","text":"client time requests qps Memory kdoctor 0.5m 238787 7959.57 350Mb hey 0.5m 7213 240.44 110Mb client time requests qps Memory kdoctor 1m 481070 8017.83 370Mb hey 1m 14665 244.42 120Mb client time requests qps Memory kdoctor 5m 2419874 8066.25 390Mb hey 5m 74776 249.25 130Mb"},{"location":"usage/performance/#netdns","title":"Netdns","text":"<p>In a pod with a CPU of 1</p> client time requests qps Memory kdoctor 1m 1855511 30925.18 23Mb dnsperf 1m 1728086 28800.40 8Mb client time requests qps Memory kdoctor 5m 9171699 30572.33 100Mb dnsperf 5m 8811137 29370.34 8Mb client time requests qps Memory kdoctor 10m 18561282 30935.47 173Mb dnsperf 10m 17260779 28767.66 8Mb"}]}